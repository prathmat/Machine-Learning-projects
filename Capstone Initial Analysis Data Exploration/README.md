# Capstone Project Initial Analysis and Data Exploration

# 🎧 Speech vs Noise Frame Classification

This project builds a machine learning pipeline to **classify short audio frames** (from noisy speech recordings) as either **speech** or **noise**, using supervised learning. It is based on the **Microsoft Scalable Noisy Speech Dataset (MS-SNSD)**.

---

## ❓ Problem Statement

In real-world environments such as cars, offices, and public places, recorded audio often contains **both speech and background noise**.

👉 The goal of this project is to:

> **Develop a machine learning model that can classify individual audio frames as either containing speech or noise.**

This classification serves as the **first critical step in noise suppression systems**, where knowing *what is noise* helps selectively attenuate it.

**Why this matters:**
- 🚘 Automotive voice assistants need to isolate driver commands from engine or traffic noise
- 🎧 Headphones with ANC need to distinguish speech from background hum or chatter
- 📞 Communication systems (VoIP, Zoom) use frame classification before applying enhancement filters

---

## 📦 Dataset Overview

### Source: [MS-SNSD GitHub Repository](https://github.com/microsoft/MS-SNSD)

The dataset is generated by mixing clean speech from **LibriSpeech** with real-world noise recordings at varying **SNR levels** (0 dB, 5 dB, 10 dB).

### Folder Structure

- `CleanSpeech_training/` — Clean speech WAV files (`clnsp1.wav`)
- `Noise_training/` — Environmental and synthetic noises (babble, street, white noise, etc.)
- `NoisySpeech_training/` — Mixed speech+noise files with naming format:
noisy1_SNRdb_5.0_clnsp1.wav

yaml
Copy
Edit

---

## 🛠️ How the Dataset Was Generated

The noisy speech samples were created by **adding clean speech to noise** at a defined SNR using this synthesis logic:

```python
from audiolib import audioread, audiowrite, snr_mixer

# Load clean and noise
clean, sr = audioread('CleanSpeech_training/clnsp1.wav')
noise, _ = audioread('Noise_training/babble_1.wav')

# Mix at 5 dB SNR
clean_adj, noise_adj, noisy = snr_mixer(clean=clean, noise=noise, snr=5)

# Save result
audiowrite(noisy, sr, 'NoisySpeech_training/noisy1_SNRdb_5.0_clnsp1.wav')
Since every noisy file is derived from a known clean file, we label frames as speech or noise by comparing their RMS energy and/or time alignment with the clean source.

🎚️ Features Used
Each frame (~20ms audio) was analyzed and converted to a set of features extracted from NoisySpeech_training, including:

Log-mel spectrogram bands (frequency energy)

RMS energy (frame loudness)

Spectral flatness (tonality vs noisiness)

Zero Crossing Rate (ZCR) (signal variation)

Spectral centroid (brightness)

Spectral rolloff (energy spread)

These features were then matched with binary labels (0 = noise, 1 = speech) derived from CleanSpeech_training.

🔍 Exploratory Data Analysis (EDA)
Key Insights:
Dataset is balanced (~60% speech, ~40% noise)

Flatness and ZCR are excellent noise indicators

Mel band energies differ significantly between speech and non-speech

Clear separation visible in distribution plots and boxplots

🧠 ML Models Trained
Model    Accuracy
✅ K-Nearest Neighbors    94.73%
✅ Random Forest    94.08%
Support Vector Machine    91.65%
Decision Tree    89.85%
Gradient Boosting    89.02%
Logistic Regression    85.58%
AdaBoost    85.51%
❌ Naive Bayes    78.81%

We selected Random Forest as the final model due to its balance of accuracy, speed, and robustness to noisy data.

📊 Feature Importance (Random Forest)
The top predictive features were:

Mid/high frequency mel bands

Spectral flatness

RMS energy

These features effectively distinguish structured speech from noise in diverse SNR conditions.

🚀 Future Work
📊 Use CNNs on spectrograms for automatic feature learning

🧠 Add LSTM layers to model temporal continuity

📈 Use pretrained models (wav2vec 2.0, HuBERT)




